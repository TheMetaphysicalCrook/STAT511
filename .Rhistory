plot(x2, y2)
xvals=data.frame(x1=seq(0, 14,by=2))
yvals=predict(fit,xvals)
## plot regression line
points(xvals$x1,yvals,type="l",col="purple",lwd=3)
xvals=data.frame(x2=seq(0, 14,by=2))
yvals=predict(fit,xvals)
## plot regression line
points(xvals$x2,yvals,type="l",col="purple",lwd=3)
## residuals
par(mfrow=c(1,2))
res=fit$resid
hist(res, main="Histogram of Residuals")
#Check for Normality
qqnorm(res)
qqline(res)
fit=lm(y2~I(x2^2), data=anscombe)
plot(x2, y2)
xvals=data.frame(x2=seq(0, 14,by=2))
yvals=predict(fit,xvals)
points(xvals$x2,yvals,type="l",col="purple",lwd=3)
fit=lm(y2~I(-x2^2), data=anscombe)
plot(x2, y2)
xvals=data.frame(x2=seq(0, 14,by=2))
yvals=predict(fit,xvals)
## plot regression line
points(xvals$x2,yvals,type="l",col="purple",lwd=3)
## residuals
fit=lm(y2~poly(x2,2), data=anscombe)
plot(x2, y2)
xvals=data.frame(x2=seq(0, 14,by=2))
yvals=predict(fit,xvals)
## plot regression line
points(xvals$x2,yvals,type="l",col="purple",lwd=3)
summary(fit)
plot(x2, y2)
xvals=data.frame(x2=seq(0, 14,by=0.5))
yvals=predict(fit,xvals)
## plot regression line
points(xvals$x2,yvals,type="l",col="purple",lwd=3)
par(mfrow=c(1,2))
res=fit$resid
hist(res, main="Histogram of Residuals")
#Check for Normality
qqnorm(res)
qqline(res)
fit=lm(y3~x3, data=anscombe)
fit=lm(y3~x3, data=anscombe)
plot(x3, y3)
xvals=data.frame(x3=seq(0, 14,by=0.5))
yvals=predict(fit,xvals)
## plot regression line
points(xvals$x3,yvals,type="l",col="purple",lwd=3)
r.star=rstudent(fit)
r.star
n=nrow(absombe)
p=1
n=nrow(ansombe)
n=nrow(anscombe)
p=1
max(r.star)
plot(r.star)
cutoff=qt(.975,df=n-p-1)
abline(h=cutoff)
abline(h=-cutoff)
fit.no.outliers=lm(y3~x3,data=anscome[-outlier.idx,])
fit.no.outliers=lm(y3~x3,data=anscombe[-outlier.idx,])
pairs(anscombe)
outlier.idx=which(abs(r.star)>cutoff)
d
outlier.idx
fit.no.outliers=lm(y3~x3,data=anscombe[-outlier.idx,])
summary(fit.no.outliers)
outlier.idx=which(abs(r.star)>cutoff)
outlier.idx
## model without outliers
fit.no.outliers=lm(y3~x3,data=anscombe[-outlier.idx,])
summary(fit.no.outliers)
plot(x3, y3)
xvals=data.frame(x3=seq(0, 14,by=0.5))
yvals=predict(fit,xvals)
## plot regression line
points(xvals$x3,yvals,type="l",col="purple",lwd=3)
r.star=rstudent(fit)
n=nrow(anscombe)
p=1
max(r.star)
plot(r.star)
cutoff=qt(.975,df=n-p-1)
abline(h=cutoff)
fit=lm(y3~x3, data=anscombe)
plot(x3, y3)
xvals=data.frame(x3=seq(0, 14,by=0.5))
yvals=predict(fit,xvals)
## plot regression line
points(xvals$x3,yvals,type="l",col="purple",lwd=3)
r.star=rstudent(fit)
n=nrow(anscombe)
p=1
outlier.idx=which(abs(r.star)>cutoff)
outlier.idx
## model without outliers
fit.no.outliers=lm(y3~x3,data=anscombe[-outlier.idx,])
summary(fit.no.outliers)
plot(x3, y3)
xvals=data.frame(x3=seq(0, 14,by=0.5))
yvals=predict(fit,xvals)
## plot regression line
points(xvals$x3,yvals,type="l",col="purple",lwd=3)
r.star=rstudent(fit)
n=nrow(anscombe)
p=1
points(xvals$x3,yvals,type="l",col="purple",lwd=3)
r.star=rstudent(fit)
n=nrow(anscombe)
p=1
max(r.star)
plot(r.star)
cutoff=qt(.975,df=n-p-1)
abline(h=cutoff)
abline(h=-cutoff)
```
## The set {x3, y3} has a linear relationship, but an outlier that wrecks havoc on our regression line!!!
## Analysis of {x3,y3} without Outliers
```{r, echo=FALSE, comment=NA}
outlier.idx=which(abs(r.star)>cutoff)
outlier.idx
## model without outliers
fit.no.outliers=lm(y3~x3,data=anscombe[-outlier.idx,])
summary(fit.no.outliers)
plot(x3, y3)
xvals=data.frame(x3=seq(0, 14,by=0.5))
yvals=predict(fit,xvals)
## plot regression line
points(xvals$x3,yvals,type="l",col="purple",lwd=3)
r.star=rstudent(fit)
n=nrow(anscombe)
p=1
max(r.star)
plot(r.star)
cutoff=qt(.975,df=n-p-1)
abline(h=cutoff)
abline(h=-cutoff)
```
outlier.idx=which(abs(r.star)>cutoff)
outlier.idx
fit.no.outliers=lm(y3~x3,data=anscombe[-outlier.idx,])
summary(fit.no.outliers)
fit=lm(y3~x3, data=anscombe)
summary(fit)
outlier.idx=which(abs(r.star)>cutoff)
outlier.idx
## model without outliers
fit.no.outliers=lm(y3~x3,data=anscombe[-outlier.idx,])
summary(fit.no.outliers)
plot(x3, y3)
xvals=data.frame(x3=seq(0, 14,by=0.5))
yvals=predict(fit.no.outliers,xvals)
## plot regression line
points(xvals$x3,yvals,type="l",col="purple",lwd=3)
r.star=rstudent(fit)
n=nrow(anscombe)
p=1
max(r.star)
plot(r.star)
cutoff=qt(.975,df=n-p-1)
abline(h=cutoff)
abline(h=-cutoff)
res=fit.no.outliers$resid
par(mfrow=c(1,2))
res=fit.no.outliers$resid
hist(res, main="Histogram of Residuals")
#Check for Normality
qqnorm(res)
qqline(res)
par(mfrow=c(1,2))
res=fit$resid
hist(res, main="Histogram of Residuals")
#Check for Normality
qqnorm(res)
qqline(res)
outlier.idx=which(abs(r.star)>cutoff)
outlier.idx
## model without outliers
fit.no.outliers=lm(y3~x3,data=anscombe[-outlier.idx,])
summary(fit.no.outliers)
plot(x3, y3)
xvals=data.frame(x3=seq(0, 14,by=0.5))
yvals=predict(fit.no.outliers,xvals)
## plot regression line
points(xvals$x3,yvals,type="l",col="purple",lwd=3)
## residuals
par(mfrow=c(1,2))
res=fit.no.outliers$resid
hist(res, main="Histogram of Residuals")
#Check for Normality
qqnorm(res)
qqline(res)
fit=lm(y4~x4, data=anscombe)
plot(x4, y4)
xvals=data.frame(x4=seq(0, 14,by=0.5))
yvals=predict(fit,xvals)
## plot regression line
points(xvals$x4,yvals,type="l",col="purple",lwd=3)
r.star=rstudent(fit)
n=nrow(anscombe)
p=1
max(r.star)
plot(r.star)
cutoff=qt(.975,df=n-p-1)
abline(h=cutoff)
abline(h=-cutoff)
## residuals
par(mfrow=c(1,2))
res=fit$resid
hist(res, main="Histogram of Residuals")
#Check for Normality
qqnorm(res)
qqline(res)
fit=lm(y4~x4, data=anscombe)
plot(x4, y4)
xvals=data.frame(x4=seq(0, 14,by=0.5))
yvals=predict(fit,xvals)
## plot regression line
points(xvals$x4,yvals,type="l",col="purple",lwd=3)
View(anscombe)
x4[8]
x4[8]
y4[8]
anscombe[8]
anscombe[(8,8)]
anscombe[8,8]
new <- x4[-8]
View(new)
anscombe[-8,]
anscombe[-c(8,3),]
anscombe[-8]
anscombe[-8,]
new_anscombe <- anscombe[-8,]
fit=lm(y4~x4, data=new_anscombe)
plot(x4, y4)
fit=lm(y4~x4, data=new_anscombe)
plot(x4, y4)
xvals=data.frame(x4=seq(0, 14,by=0.5))
yvals=predict(fit,xvals)
## plot regression line
points(xvals$x4,yvals,type="l",col="purple",lwd=3)
plot(new_anscomebe$x4, new_anscombe$y4)
plot(new_anscombe$x4, new_anscombe$y4)
xvals=data.frame(new_anscombe$x4=seq(0, 14,by=0.5))
xvals=data.frame(new_anscombex4=seq(0, 14,by=0.5))
yvals=predict(fit,xvals)
points(xvals$x4,yvals,type="l",col="purple",lwd=3)
xvals=data.frame(new_anscombe$x4=seq(0, 14,by=0.5))
new_anscombe
plot(new_anscombe$x4, new_anscombe$y4)
xvals=data.frame(x4=seq(0, 14,by=0.5))
yvals=predict(fit,xvals)
## plot regression line
points(xvals$x4,yvals,type="l",col="purple",lwd=3)
r.star=rstudent(fit)
n=nrow(anscombe)
p=1
max(r.star)
plot(r.star)
cutoff=qt(.975,df=n-p-1)
abline(h=cutoff)
abline(h=-cutoff)
## residuals
par(mfrow=c(1,2))
res=fit$resid
hist(res, main="Histogram of Residuals")
#Check for Normality
qqnorm(res)
qqline(res)
rnorm(10)
rnorm(10)
y <- rnorm(10)
qqnorm(y)
qqline(y)
par(mfrow=c(1,2))
res=fit$resid
hist(res, main="Histogram of Residuals")
a <- rnorm(10)
b <- rnorm(10)
c <- rnorm(10)
d <- rnorm(10)
par(mfrow=c(1,2))
qqnorm(a)
qqline(a)
qqnorm(b)
qqline(b)
qqnorm(c)
qqline(c)
qqnorm(d)
qqline(d)
fit=lm(dist/speed~speed,data=cars)
summary(fit)$coeff
attach(cars)
y<-dist
x_design<-cbind(rep(1,nrow(cars)),cars$speed) #cars$speed^2)
x_0<-c(1,30) #30^2)
x_0<-cbind(rep(1,351),seq(0, 35, 0.1)) # seq(0, 35, 0.1)^2))
n<-nrow(x_design)
p<-ncol(x_design)
hatmat<-x_design%*%solve(t(x_design)%*%x_design)%*%t(x_design)
res <- (diag(1,nrow(x_design)) - hatmat)%*%y
sigma_hat<-(t(res)%*%res)/(n-p)
beta_hat<-solve(t(x_design)%*%x_design)%*%t(x_design)%*%y
y_hat_0<-x_0%*%beta_hat
var_beta_hat<-as.numeric(sigma_hat)*(x_0)%*%solve(t(x_design)%*%x_design)%*%t(x_0)
se_betahat<-sqrt(diag(var_beta_hat))
#### Calculate Confidence Interval for the mean
high95<-y_hat_0+qt(0.975,n-p)*se_betahat
low95<-y_hat_0+qt(0.025,n-p)*se_betahat
# the confidence bands
# lines(x, fitted[, "lwr"], lty = "dotted")
#   lines(x, fitted[, "upr"], lty = "dotted")
high95
fit=lm(dist~speed+I(speed^2),data=cars)
attach(cars)
y<-dist
x_design<-cbind(rep(1,nrow(cars)),cars$speed,cars$speed^2)
x_0<-c(1,30,30^2)
x_0<-(cbind(rep(1,351),seq(0, 35, 0.1),seq(0, 35, 0.1)^2))
n<-nrow(x_design)
p<-ncol(x_design)
hatmat<-x_design%*%solve(t(x_design)%*%x_design)%*%t(x_design)
res<-(diag(1,nrow(x_design))-hatmat)%*%y
sigma_hat<-(t(res)%*%res)/(n-p)
beta_hat<-solve(t(x_design)%*%x_design)%*%t(x_design)%*%y
y_hat_0<-x_0%*%beta_hat
var_beta_hat<-as.numeric(sigma_hat)*(x_0)%*%solve(t(x_design)%*%x_design)%*%t(x_0)
se_betahat<-sqrt(diag(var_beta_hat))
high95<-y_hat_0+qt(0.975,n-p)*se_betahat
low95<-y_hat_0+qt(0.025,n-p)*se_betahat
plot(speed, dist)
xvals=data.frame(speed=seq(0, 25 ,by=0.5))
yvals=predict(fit,xvals)
## plot regression line
points(xvals$speed,yvals,type="l",col="purple",lwd=3)
lines(high95, lty = "dotted")
lines(lwo95, lty = "dotted")
lines(low95, lty = "dotted")
points(xvals$speed,yvals,type="l",col="purple",lwd=3)
# the confidence bands
lines(high95, lty = "dotted")
lines(low95, lty = "dotted")
plot(speed, dist)
xvals=data.frame(speed=seq(0, 25 ,by=0.5))
yvals=predict(fit,xvals)
## plot regression line
points(xvals$speed,yvals,type="l",col="purple",lwd=3)
# the confidence bands
lines(high95, lty = "dotted")
lines(low95, lty = "dotted")
low95<-y_hat_0-qt(0.025,n-p)*se_betahat
lines(low95, lty = "dotted")
low95
plot(speed, dist)
xvals=data.frame(speed=seq(0, 25 ,by=0.5))
yvals=predict(fit,xvals)
## plot regression line
points(xvals$speed,yvals,type="l",col="purple",lwd=3)
lines(low95, lty = "dotted")
lines(high95, yvals, lty = "dotted")
lines(high95, xvals, lty = "dotted")
lines(high95, fit, lty = "dotted")
xnew <- seq(0,400)
int <- confint(fit)
lines(xnew, (int[1,2]+int[2,1]*xnew))
lines(xnew, (int[1,1]+int[2,2]*xnew))
plot(speed, dist)
xvals=data.frame(speed=seq(0, 25 ,by=0.5))
yvals=predict(fit,xvals)
points(xvals$speed,yvals,type="l",col="purple",lwd=3)
xnew <- seq(0,400)
int <- confint(fit)
lines(xnew, (int[1,2]+int[2,1]*xnew))
lines(xnew, (int[1,1]+int[2,2]*xnew))
require(visreg)
install.packages("visreg")
library(visreg)
require(visreg)
visreg(fit)
xnew <- seq(0,400)
int <- confint(fit)
lines(xnew, (int[1,2]+int[2,1]*xnew))
lines(xnew, (int[1,1]+int[2,2]*xnew))
points(xvals$speed,yvals,type="l",col="purple",lwd=3)
# the confidence bands
require(visreg)
visreg(fit)
xnew <- seq(0,400)
plot(speed, dist)
xvals=data.frame(speed=seq(0, 25 ,by=0.5))
yvals=predict(fit,xvals)
## plot regression line
points(xvals$speed,yvals,type="l",col="purple",lwd=3)
# the confidence bands
require(visreg)
visreg(fit)
xnew <- seq(0,400)
mean(high95)
mean(low95)
high95<-y_hat_0+qt(0.975,n-p)*se_betahat
View(high95)
fit=lm(dist~speed+I(speed^2),data=cars)
attach(cars)
y<-dist
View(y)
x_design<-cbind(rep(1,nrow(cars)),cars$speed,cars$speed^2)
View(x_design)
x_0<-c(1,30,30^2)
View(x_0)
x_0<-(cbind(rep(1,351),seq(0, 35, 0.1),seq(0, 35, 0.1)^2))
View(x_0)
n<-nrow(x_design)
n
p<-ncol(x_design)
p
hatmat<-x_design%*%solve(t(x_design)%*%x_design)%*%t(x_design)
View(hatmat)
res<-(diag(1,nrow(x_design))-hatmat)%*%y
View(res)
sigma_hat<-(t(res)%*%res)/(n-p)
View(sigma_hat)
beta_hat<-solve(t(x_design)%*%x_design)%*%t(x_design)%*%y
beta_hat
t(x_0)
rnorm(100)
p <- rnorm(100)
mean(p)
sd(p)
p <- rnorm(10)
mean(p)
sd(p)
setwd("/Users/benStraub/Desktop/STAT511")
Munich = read.csv("rent99.raw", sep="")
fit=lm(rentsqm~I(1/area)+factor(location),data=Munich)
# Sigma Hat Squared form original model
s2.hat=(summary(fit)$sigma)^2
# Coefficients of Model
Beta_4 <- fit$coefficients[4]
Beta_3 <- fit$coefficients[3]
Super_Beta <- Beta_4 - Beta_3
# ???Unsu
X=model.matrix(fit)
Xt = t(X)
invXXt = solve(Xt%*%X)
diag_general <- diag(invXXt)
diag_4 <- as.numeric(diag_general[4])
diag_3 <- as.numeric(diag_general[3])
# Make sure to subtract the covariance
test_Statistic <- as.numeric((Super_Beta)/sqrt(s2.hat*(diag_4 + diag_3 - 2*invXXt[4,3])))
test_Statistic
fit=lm(dist/speed~speed, data=cars)
coef=summary(fit)$coefficients[2,1]
err=summary(fit)$coefficients[2,2]
coef + c(-1,1)*err*qt(0.975, 48)
fit=lm(y1~x1, data=anscombe)
kable(summary(fit)$coeff, digits=3, caption = "x1&y1")
summary(fit)
View(fit)
summary(fit)%coeff
summary(fit)$coeff
summary(fit)$r.squared
fit=lm(y1~x1, data=anscombe)
kable(summary(fit)$coeff, digits=3, caption = "x1&y1")
summary(fit)$r.squared
fit=lm(y2~x2, data=anscombe)
kable(summary(fit)$coeff, digits=3, caption = "x2&y2")
summary(fit)$r.squared
fit=lm(y3~x3, data=anscombe)
kable(summary(fit)$coeff, digits=3, caption = "x3&y3")
summary(fit)$r.squared
fit=lm(y4~x4, data=anscombe)
kable(summary(fit)$coeff, digits=3, caption = "x4&y4")
summary(fit)$r.squared
summary(fit)
predict(fit, newdata=anscombe, interval='confidence')
fit=lm(y1~x1, data=anscombe)
predict(fit, data.frame(train_x = c(13))).```
predict(fit, data.frame(train_x = c(13)).
predict(fit, data.frame(train_x = c(13))
predict(fit, data.frame(train_x = c(13)))
predict(fit, data.frame(train_x = 13)
predict(fit, data.frame(train_x = 13))
predict(fit, 13)
predict(fit, newdata=data.frame(x1=13))
predict_1 <- predict(fit, newdata=data.frame(x1=13))
fit=lm(y2~poly(x2,2), data=anscombe)
predict_2 <- predict(fit, newdata=data.frame(x1=13))
fit=lm(y2~poly(x2,2), data=anscombe)
predict_2 <- predict(fit, newdata=data.frame(x2=13))
predict_2
predict_3 <- predict(fit, newdata=data.frame(x3=13))
fit.no.outliers=lm(y3~x3,data=anscombe[-outlier.idx,])
predict_3 <- predict(fit, newdata=data.frame(x3=13))
predict_3
outlier.idx=which(abs(r.star)>cutoff)
outlier.idx
fit.no.outliers=lm(y3~x3,data=anscombe[-outlier.idx,])
r.star=rstudent(fit)
n=nrow(anscombe)
p=1
max(r.star)
cutoff=qt(.975,df=n-p-1)
outlier.idx=which(abs(r.star)>cutoff)
fit.no.outliers=lm(y3~x3,data=anscombe[-outlier.idx,])
fit=lm(dist~speed,data=cars)
kable(summary(fit)$coeff, digits=3)
